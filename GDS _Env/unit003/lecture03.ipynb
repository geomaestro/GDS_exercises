{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc759772",
   "metadata": {},
   "source": [
    "<div class=\"frontmatter text-center\">\n",
    "<h1>Geospatial Data Science</h1>\n",
    "<h2>Lecture 3: Choropleth Mapping</h2>\n",
    "<h3>IT University of Copenhagen, Spring 2022</h3>\n",
    "<h3>Instructor: Michael Szell</h3>\n",
    "</div>\n",
    "\n",
    "# Source\n",
    "This notebook was adapted from:\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bD/lab_D.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c6da7-f1c7-4a25-b9d7-e73563f5b20a",
   "metadata": {},
   "source": [
    "In this session, we will build on all we have learnt so far about loading and manipulating (spatial) data and apply it to one of the most commonly used forms of spatial analysis: choropleths. Remember these are maps that display the spatial distribution of a variable encoded in a color scheme, also called *palette*. Although there are many ways in which you can convert the values of a variable into a specific color, we will focus in this context only on a handful of them, in particular:\n",
    "\n",
    "* Unique values\n",
    "* Equal interval\n",
    "* Quantiles\n",
    "* Fisher-Jenks\n",
    "\n",
    "Before going into the code, let's get a feel for the different classification schemes: https://mgimond.github.io/Spatial/symbolizing-features.html#an-interactive-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a779e3d-759f-42aa-8c6e-787c9d5df54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from pysal.lib import examples\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pysal.viz import mapclassify\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5aa386-5694-4dd3-aff5-101f92881d18",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To mirror the [original chapter](https://geographicdata.science/book/notebooks/05_choropleth.html) this section is based on, we will use the same dataset: the [Mexico GDP per capita dataset](https://geographicdata.science/book/data/mexico/README.html), which we can access as a PySAL example dataset. You can read more about PySAL example datasets [here](https://pysal.org/libpysal/notebooks/examples.html).\n",
    "\n",
    "We can get a short explanation of the dataset through the `explain` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131a71f-11e0-4064-ab96-4b436ecf015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.explain(\"mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3cd78-e4a8-4702-aa34-f985e2659415",
   "metadata": {},
   "source": [
    "Now, to download it from its remote location, we can use `load_example`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31302e7-4320-46b5-a505-998e6ab29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = examples.load_example(\"mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bd96a-079b-4387-ac97-6baaf4c07da0",
   "metadata": {},
   "source": [
    "This will download the data and place it on your home directory. We can inspect the directory where it is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6b5c8-acfe-4c54-9312-37bb16dd2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.get_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305f06c-ddfe-4051-8d2c-fcc9301de93c",
   "metadata": {},
   "source": [
    "For this section, we will read the ESRI shapefile, which we can do by pointing `geopandas.read_file` to the `.shp` file. The utility function `get_path` makes it a bit easier for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdb394-c7d9-456d-8af1-9795f4d3c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = geopandas.read_file(examples.get_path(\"mexicojoin.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b2668-2e6d-4775-8ef1-acc6e117e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73883c-2af7-4e78-8dfe-f74185aacbf2",
   "metadata": {},
   "source": [
    "And, from now on, `db` is a table as we are used to so far in this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a9d6e-d970-4626-91e1-68f30ad8e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63e378-f846-4836-9c5a-cf994edf1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee87882-2624-4231-abc1-58c2484d48fd",
   "metadata": {},
   "source": [
    "The data however does not include a CRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120210b9-dbc6-4473-8270-a71fa9965e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc93c8-34ec-4987-a9e2-cd9e2e57e611",
   "metadata": {},
   "source": [
    "To be able to add baselayers, we need to specify one. Looking at the details and the original reference, we find the data are expressed in longitude and latitude, so the CRS we can use is `EPSG:4326`. Let's add it to `db`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b84210-589f-4d87-abd0-e6601b5f6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.crs = \"EPSG:4326\"\n",
    "db.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d9759-6b4c-4dc5-8df9-3e9f1790a045",
   "metadata": {},
   "source": [
    "Now we are fully ready to map!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc64161-9923-4f9d-b7d4-792ffb952930",
   "metadata": {},
   "source": [
    "## Choropleths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b08d7b-cc6d-4ccf-a2ed-8bc85fe8ed8a",
   "metadata": {},
   "source": [
    "### Unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f3fd-8b10-4c39-aba1-ce291dbcb81b",
   "metadata": {},
   "source": [
    "A choropleth for categorical variables simply assigns a different color to every potential value in the series. The main requirement in this case is then for the color scheme to reflect the fact that different values are not ordered or follow a particular scale.\n",
    "\n",
    "In Python, creating categorical choropleths is possible with one line of code. To demonstrate this, we can plot the Mexican states and the region each belongs to based on the Mexican Statistics Institute (coded in our table as the `INEGI` variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7d84a-268c-4dda-8022-dba9524ab9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"INEGI\", \n",
    "    categorical=True, \n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f6647-3d43-4145-b742-f246211b84f8",
   "metadata": {},
   "source": [
    "Let us stop for a second in a few crucial aspects:\n",
    "\n",
    "* Note how we are using the same approach as for basic maps, the command `plot`, but we now need to add the argument `column` to specify which column in particular is to be represented.\n",
    "* Since the variable is categorical we need to make that explicit by setting the argument `categorical` to `True`.\n",
    "* As an optional argument, we can set `legend` to `True` and the resulting figure will include a legend with the names of all the values in the map.\n",
    "* Unless we specify a different colormap, the selected one respects the categorical nature of the data by not implying a gradient or scale but a qualitative structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d8fd5-c439-4e46-8f99-19ccd12d1ff6",
   "metadata": {},
   "source": [
    "### Equal interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81ef4b-009c-40fe-b30a-5785eab08642",
   "metadata": {},
   "source": [
    "If, instead of categorical variables, we want to display the geographical distribution of a continuous phenomenon, we need to select a way to encode each value into a color. One potential solution is applying what is usually called \"equal intervals\". The intuition of this method is to split the *range* of the distribution, the difference between the minimum and maximum value, into equally large segments and to assign a different color to each of them according to a palette that reflects the fact that values are ordered.\n",
    "\n",
    "Creating the choropleth is relatively straightforward in Python. For example, to create an equal interval on the GDP per capita in 2000 (`PCGDP2000`), we can run a similar command as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ca51d-ae60-4a32-8dda-52c72ce960e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"equal_interval\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ff936-0a4a-4d0b-b849-23bc3f08bece",
   "metadata": {},
   "source": [
    "Pay attention to the key differences:\n",
    "\n",
    "* Instead of specifyig `categorical` as `True`, we replace it by the argument `scheme`, which we will use for all choropleths that require a continuous classification scheme. In this case, we set it to `equal_interval`.\n",
    "* As above, we set the number of colors to 7. Note that we need not pass the bins we calculated above, the plotting method does it itself under the hood for us.\n",
    "* As optional arguments, we can change the colormap to a yellow to green gradient, which is one of the recommended ones by [ColorBrewer](http://colorbrewer2.org/) for a sequential palette. \n",
    "\n",
    "Now, let's dig a bit deeper into the classification, and how exactly we are encoding values into colors. Each segment, also called bins or buckets, can also be calculated using the library `mapclassify` from the `PySAL` family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb56276-b637-4add-9042-d4ebd310b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.EqualInterval(db[\"PCGDP2000\"], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dff59-88f4-4162-b33a-7bc3dd57d936",
   "metadata": {},
   "source": [
    "The only additional argument to pass to `Equal_Interval`, other than the actual variable we would like to classify is the number of segments we want to create, `k`, which we are arbitrarily setting to seven in this case. This will be the number of colors that will be plotted on the map so, although having several can give more detail, at some point the marginal value of an additional one is fairly limited, given the ability of the brain to tell any differences.\n",
    "\n",
    "Once we have classified the variable, we can check the actual break points where values stop being in one class and become part of the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8b1d8-1d18-48b6-a84b-34b5e2d6c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5b09c-51e6-4b14-934d-0557ba9d60f8",
   "metadata": {},
   "source": [
    "The array of breaking points above implies that any value in the variable below 15,207.57 will get the first color in the gradient when mapped, values between 15,207.57 and 21,731.14 the next one, and so on.\n",
    "\n",
    "The key characteristic in equal interval maps is that the bins are allocated based on the magnitude on the values, irrespective of how many obervations fall into each bin as a result of it. In highly skewed distributions, this can result in bins with a large number of observations, while others only have a handful of outliers. This can be seen in the summary table printed out above, where ten states are in the first group, but only one of them belong to the one with highest values. This can also be represented visually with a kernel density plot where the break points are included as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42074bdc-5888-4dad-9f57-e27869430dcc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db[\"PCGDP2000\"], shade=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df80363-47d7-45d5-975a-de89fd331c8f",
   "metadata": {},
   "source": [
    "Technically speaking, the figure is created by overlaying a KDE plot with vertical bars for each of the break points. This makes much more explicit the issue highlighed by which the first bin contains a large amount of observations while the one with top values only encompasses a handful of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4513072-d53f-48cb-b0e7-c2af7ca4dd47",
   "metadata": {},
   "source": [
    "### Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca577e-f0fe-4557-b055-1001f3c838f0",
   "metadata": {},
   "source": [
    "One solution to obtain a more balanced classification scheme is using quantiles. This, by definition, assigns the same amount of values to each bin: the entire series is laid out in order and break points are assigned in a way that leaves exactly the same amount of observations between each of them. This \"observation-based\" approach contrasts with the \"value-based\" method of equal intervals and, although it can obscure the magnitude of extreme values, it can be more informative in cases with skewed distributions.\n",
    "\n",
    "The code required to create the choropleth mirrors that needed above for equal intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c4e4e-a11e-40cd-a736-aa4bd3a50f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5aa90-7682-487a-8b28-7d7c23fbaa5b",
   "metadata": {},
   "source": [
    "Note how, in this case, the amount of polygons in each color is by definition much more balanced (almost equal in fact, except for rounding differences). This obscures outlier values, which get blurred by significantly smaller values in the same group, but allows to get more detail in the \"most populated\" part of the distribution, where instead of only white polygons, we can now discern more variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96131e-9916-4f8d-b2e9-f61a95458a04",
   "metadata": {},
   "source": [
    "To get further insight into the quantile classification, let's calculate it with `mapclassify`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f7437-f412-4d75-bdaa-3fa8c42a97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.Quantiles(db[\"PCGDP2000\"], k=4)\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7edc0c-78dd-4171-9852-ea63cf045e6a",
   "metadata": {},
   "source": [
    "And, similarly, the bins can also be inspected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122da94-cdf2-4edc-a361-74de5d30c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94505285-3715-4728-a0f0-27fedac2d1a8",
   "metadata": {},
   "source": [
    "The visualization of the distribution can be generated in a similar way as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6ae6a-43c2-486f-8614-f9dd332730aa",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db[\"PCGDP2000\"], shade=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13133f20-2342-4f8e-9aec-aaf11a7110ae",
   "metadata": {},
   "source": [
    "### Fisher-Jenks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f3748-2ec3-4e4c-9d50-f48194952180",
   "metadata": {},
   "source": [
    "Equal interval and quantiles are only two examples of very many classification schemes to encode values into colors. Although not all of them are integrated into `geopandas`, `PySAL` includes several other classification schemes (for a detailed list, have a look at this [link](https://pysal.org/mapclassify/notebooks/01_maximum_breaks.html)). As an example of a more sophisticated one, let us create a Fisher-Jenks choropleth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92968d2-df0c-42c4-bcef-a39671076cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"fisher_jenks\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a3281-96d1-43da-b21e-8e72af1a0f6a",
   "metadata": {},
   "source": [
    "The same classification can be obtained with a similar approach as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f83dc4-a1b4-493c-a93d-f87e23bfa428",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.FisherJenks(db[\"PCGDP2000\"], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158e564-2ae9-4462-826a-ec19a7a928d7",
   "metadata": {},
   "source": [
    "This methodology aims at minimizing the variance *within* each bin while maximizing that *between* different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f975e58-d14a-43f5-9e4a-89f2de0ce015",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3e79c-05f1-4ec0-9529-a90bd96e34a5",
   "metadata": {},
   "source": [
    "Graphically, we can see how the break points are not equally spaced but are adapting to obtain an optimal grouping of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66462aad-c0c6-4837-bb94-83de4db79ccf",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db[\"PCGDP2000\"], shade=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e9b70-59f5-405d-83dd-74fd4d9d3d6f",
   "metadata": {},
   "source": [
    "For example, the bin with highest values covers a much wider span that the one with lowest, because there are fewer states in that value range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843ec10-1eef-44e4-bd24-eebbc77b11ed",
   "metadata": {},
   "source": [
    "## Zooming into the map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b5467-a00a-4826-8cb8-145e7d6adf75",
   "metadata": {},
   "source": [
    "### Zoom into full map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a282c-a01e-43ab-bed4-bcf6a37ac2f1",
   "metadata": {},
   "source": [
    "A general map of an entire region, or urban area, can sometimes obscure local patterns because they happen at a much smaller scale that cannot be perceived in the global view. One way to solve this is by providing a focus of a smaller part of the map in a separate figure. Although there are many ways to do this in Python, the most straightforward one is to reset the limits of the axes to center them in the area of interest.\n",
    "\n",
    "As an example, let us consider the quantile map produced above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb854706-d514-447a-9b8b-6199fc4ec417",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d484e6-dccc-4896-b39c-e6dd81c25aca",
   "metadata": {},
   "source": [
    "If we want to focus around the capital, Mexico DF, the first step involves realising that such area of the map (the little dark green polygon in the SE centre of the map), falls within the coordinates of -102W/-97W, and 18N/21N, roughly speaking. To display a zoom map into that area, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50e909-e248-46bc-bcb8-62c16792eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Draw the choropleth\n",
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=False,\n",
    "    ax=ax\n",
    ")\n",
    "# Redimensionate X and Y axes to desired bounds\n",
    "ax.set_ylim(18, 21)\n",
    "ax.set_xlim(-102, -97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bee90-9365-445d-9f7e-b1996677b7bd",
   "metadata": {},
   "source": [
    "### Partial map\n",
    "\n",
    "The approach above is straightforward, but not necessarily the most efficient one: not that, to generate a map of a potentially very small area, we effectively draw the entire (potentially very large) map, and discard everything except the section we want. This is not straightforward to notice at first sight, but what Python is doing in the code cell above is plottin the entire `db` object, and only then focusing the figure on the X and Y ranges specified in `set_xlim`/`set_ylim`.\n",
    "\n",
    "Sometimes, this is required. For example, if we want to retain the same coloring used for the national map, but focus on the region around Mexico DF, this approach is the easiest one.\n",
    "\n",
    "However, sometimes, we only need to plot the _geographical features_ within a given range, and we either don't need to keep the national coloring (e.g. we are using a single color), or we want a classification performed _only_ with the features in the region.\n",
    "\n",
    "For these cases, it is computationally more efficient to select the data we want to plot first, and then display them through `plot`. For this, we can rely on the `cx` operator (coordinate based indexing): https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.cx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d24e6-e148-46a3-8384-1cb64a544a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = db.cx[-102:-97, 18:21]\n",
    "subset.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe5dc4-e95f-49d8-91ce-dc2045f9446c",
   "metadata": {},
   "source": [
    "We query the range of spatial coordinates similarly to how we query indices with `loc`. Note however the result includes full geographic features, and hence the polygons with at least some area within the range are included fully. This results in a larger range than originally specified.\n",
    "\n",
    "This approach is a \"spatial slice\", where instead of subsetting by indices of the table, slices are based on the spatial coordinates of the data represented in the table.\n",
    "\n",
    "Since the result is a `GeoDataFrame` itself, we can create a choropleth that is based only on the data in the subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3abb9a-e7c9-4922-b9d5-971225ca60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
